Data-Lakehouse-and-Analytics-for-Quick-Commerce

A complete end-to-end Big Data Lakehouse pipeline for a Quick-Commerce platform.
This project simulates item catalog generation, synthetic order streaming, multi-layer data processing (Bronze â†’ Silver â†’ Gold), Hive analysis, and RNN-based demand forecasting using PyTorch.

ğŸ“‚ Project Structure
BIG_DATA_PROJECT/
â”‚
â”œâ”€â”€ Bash_scripts/
â”‚   â”œâ”€â”€ reset.bash
â”‚   â””â”€â”€ start.bash
â”‚
â”œâ”€â”€ Bronze layer/
â”‚   â”œâ”€â”€ consumers/
â”‚   â”‚   â””â”€â”€ orders_save_consumer.py
â”‚   â”œâ”€â”€ Extras/
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ (Schemas for bronze order ingestion)
â”‚
â”œâ”€â”€ Silver_layer/
â”‚   â”œâ”€â”€ silver_consumer.py
â”‚   â””â”€â”€ silver_analysis.py
â”‚
â”œâ”€â”€ Golden layer/
â”‚   â”œâ”€â”€ item_categories_per_city.py
â”‚   â”œâ”€â”€ data_for_rnn.py
â”‚   â””â”€â”€ rnn_data_read.py
â”‚
â”œâ”€â”€ Producers/
â”‚   â”œâ”€â”€ generate_item_catalog.py
â”‚   â”œâ”€â”€ item_catalog.json
â”‚   â””â”€â”€ order_producer.py
â”‚
â”œâ”€â”€ Pytorch/
â”‚   â””â”€â”€ rnn.py
â”‚
â”œâ”€â”€ Hive/
â”‚   â””â”€â”€ (Hive queries for analytical workloads)
â”‚
â”œâ”€â”€ Jars/
â”‚   â””â”€â”€ (Delta Lake & Kafka JARs)
â”‚
â”œâ”€â”€ flow.txt
â”œâ”€â”€ item_catalog.json
â””â”€â”€ README.md

ğŸ“ Overview

This project builds a full data lakehouse pipeline for a Quick-Commerce business.
It uses:

Kafka (streaming orders)

Spark Structured Streaming (ETL: Bronze â†’ Silver â†’ Gold)

Delta Lake (storage & versioning)

Hive (analytics)

PyTorch RNN (demand forecasting)

ğŸ”§ Bash Scripts
1ï¸âƒ£ reset.bash

Deletes all checkpoints and table data from project storage.

Useful for resetting the entire lakehouse pipeline.

2ï¸âƒ£ start.bash

Starts all necessary services:

Zookeeper

Kafka Broker

Spark Master

Hive Metastore

Beeline

ğŸ“¤ Producers
generate_item_catalog.py

Generates Item Dimension / Catalog

Saves catalog to:

Project folder

HDFS (for downstream pipelines)

order_producer.py

Produces synthetic orders (time-series based)

Publishes messages to Kafka topic: order_placed_bronze

ğŸ¥‰ Bronze Layer
orders_save_consumer.py

Kafka consumer for raw order events

Stores raw unprocessed data in Bronze Delta tables

This is the first consumer to run

Schemas Folder

Contains schema definitions for bronze order ingestion.

ğŸ¥ˆ Silver Layer
silver_consumer.py

Reads data from Bronze tables

Performs:

Cleaning

Transformation

Standardization

Stores processed output back into Silver Delta tables

silver_analysis.py

Performs aggregations on Silver tables

Prepares aggregated datasets for RNN training

ğŸ¥‡ Golden Layer
item_categories_per_city.py

Joins Silver data into Gold-level aggregated categories

Generates metrics: item availability per city

data_for_rnn.py

Creates 5-minute windowed timeframes

Produces training sequences for RNN forecasting

Saves data as Delta/JSON for the ML pipeline

rnn_data_read.py

Reads the prepared Gold dataset

Used before RNN model training

ğŸ¤– PyTorch RNN
rnn.py

RNN model for demand forecasting

Uses sliding window data from Gold layer

Predicts future order volume per category/city

ğŸ Hive

Contains Hive SQL scripts:

Exploratory analysis

Business intelligence queries

Report generation over Gold/Silver tables

ğŸ“¦ Jars

Includes:

Delta Lake JARs

Kafka JARs

Other dependencies used by Spark jobs

ğŸ“˜ flow.txt

Summary of complete project architecture

Data movement sequence

Component interactions

ğŸ—‚ item_catalog.json

Static export of the generated item catalog.